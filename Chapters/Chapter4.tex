% Chapter 1

\chapter{A real-time implementation for the Odroid-XU developer board}

\lhead{Chapter 5. \emph{A real-time implementation}} % This is for the header on each page - perhaps a shortened title

%----------------------------------------------------------------------------------------
Another important contribution of this thesis is a real-time implementation of our gesture recognition approach, ready to be used on an embedded device and suitable for real-world applications. To achieve this goal, we will need to modify and improve the classification module of our algorithm, and choose the Odroid-XU developer board as our target plaform. To make our implementation capable of running in real-time, we will also have to exploit several optimization techniques. At the end of this chapter we will also present some real-world applications of the proposed algorithm.

In the previous chapter, we classified frame sequences containing a gesture using a linear and power-normalized SVM classifier. In particular, we extracted trajectories with fixed length $L=30$ from the frame sequence, we then applied a standard BoW approach, and the output of the BoW was our final feature vector, with fixed size and ready to be classified. We now ask ourselves how to modify this approach in order to take a frame stream (with unknown length) as input, instead of a fixed sized sequence. To this aim, we propose two approaches: a naive sliding window approach and a more complex label sequence learning approach. In the following, we will use the latter as our default implementation.

\section{How to treat a frame stream}
The simplest way to extend the proposed approach to frame streams is to exploit a sliding window. Let's suppose we have a window with size $W$, where $W$ is greater than the maximum gesture duration, and that the window step is $s$. Thus, at iteration $i$, the window will contain the following set of frames:
\begin{equation}
\{ I_{1+i\cdot s}, I_{2+i \cdot s}, ..., I_{W+i \cdot s} \}
\end{equation}

each window is then classified using the linear SVM classifier. Of course, in this case we have an additional class, the \textit{non-gesture} one. We define as \textit{non-gesture} any window that does not contain a complete gesture.

However, this implementation leads to, at least, three drawbacks: first, we would be always $s$ frames late, and therefore for a real-time gesture recognition we would have to choose a small $s$ and accept to classify the same frame lots of times. Secondly, the classifier wouldn't learn a clear concept of \textit{non-gesture}, since the \textit{non-gesture} class would contain partial gestures. Third, an SVM classifier doesn't learn label/label dependencies.

\medskip

For this reasons, we will now treat the problem of classifying a frame stream as a \textit{label sequence learning} problem. We start observing that our trajectory descriptors can be thought as a concatenation of single-point descriptors. Formally, let's consider a trajectory $T_i$ starting at frame $t_i$ and ending at frame $t_i+L-1$: we can express $T_i$ as $\left[ P_{t_i}^i, P_{t_{i+1}}^i, ..., P_{t_i+L-1}^i \right]$, where $P_{j}^i$ is the point of trajectory $T_i$ at frame $j$.  If we denote with $D(T_i)$ the descriptor of trajectory $T_i$, then $D(T_i)$ can be expressed as the concatenation of $d(P_{t_i}^i), d(P_{t_{i+1}}^i), ..., d( P_{t_i+L-1}^i)$, where $d(P_{j}^i)$ is a descriptor computed around $P_j^i$, at frame $j$. Proving this statement is straightforward, given the nature of the descriptors employed.

Let's now consider a single frame $j$ of a frame stream, and suppose $ \{ T_1, T_2, ..., T_n\}$ are the trajectories crossing this frame. In this on-line version, we build a BoW histogram on each frame, being the input of the BoW clustering the set of descriptors of the trajectory points crossing the frame, i.e. $\{ d(P_{j}^1),  d(P_{j}^2), ...,  d(P_{j}^n) \}$.  Therefore, the corresponding BoW histogram won't represent a collection of trajectories anymore, but a collection of points, each one belonging to a different trajectory, and all related to the same frame.

Shortly, in the previous chapter the temporal dimension was encoded directly into the BoW input, now this will be up to the sequence classifier.


\subsection{Classification}
 In Section \ref{svmhmm} we described how a Struct SVM classifier can be used for label sequence learning. Now, we exploit the Structural SVM implementation by Altun \etal \cite{altun2003hidden} to classify this modified version of our feature vectors.

 First of all, we modify the source code provided by Joachims \cite{joachims} in order to include higher order label-label and label-features interactions, so that the generic 
$\Psi(\mathbf{x},\mathbf{y})$ takes the following form:

\begin{equation}
\Psi_{\epsilon,\tau}(\mathbf{x},\mathbf{y}) = \left( \begin{array}{cc} \sum_{t=1}^T \Phi(\mathbf{x}^t) \otimes  \Lambda^c(y^{t-\epsilon}) \otimes ... \otimes \Lambda^c(y^t) \\ \eta\sum_{t=1}^{T}  \Lambda^c(y^{t-\tau}) \otimes ... \otimes \Lambda^c(y^t) \end{array} \right)
\end{equation}

where $\epsilon$ is the order of label-label dependencies, and $\tau$ is the order of label-features dependencies.

Furthermore, we investigate the use of different loss functions and their impact on recognition rates and confusion matrices. We define a generic loss function that exploits a per-token loss function $\delta(y^t,y'^t)$: $\Delta(\mathbf{y},\mathbf{y'}) = \sum_{t=1}^T \delta(y^t,y'^t)$, and propose two different per-token loss functions:

\begin{equation}
\delta_0(y,y') = \begin{cases} 0 \quad \text{if } y= y'  \\ 
1 \quad \text{otherwise}\end{cases}
\label{eq:loss-default}
\end{equation}

\begin{equation}
\delta_1(y,y') = \begin{cases} 0 \quad \text{if } y= y'  \\ 
3 \quad \text{if } y=y_{ng}, y' \neq y_{ng} \lor y\neq y_{ng}, y'=y_{ng}  \\
1 \quad \text{otherwise}\end{cases}
\label{eq:loss-mia}
\end{equation}

where $y_{ng}$ is the \textit{non-gesture} label. \ref{eq:loss-default} is the default 0-1 loss function included in Joachims's code, and \ref{eq:loss-mia} is a slightly modifed version that aims at penalizing the case when a non-gesture frame is confused with a gesture frame or viceversa.

\begin{table}
\begin{center}
\begin{tabular}{|c|c|c|}
\hline
$\mathbf{\tau}$ & $\mathbf{\epsilon}$ & \textbf{Accuracy \%} \\
\hline
\hline
1							& 0							& 92.2 \% \\
1							& 1							& 92.4 \% \\
2							& 0							& 92.4 \% \\
2							& 1							& 93.0 \% \\
\textbf{2}						& \textbf{2}						& \textbf{93.8 \%} \\
\hline
\end{tabular}
\end{center}
\caption{Accuracy results with different orders of dependencies of transitions and emissions}
\label{transemis}
\end{table}

\begin{table*}
\begin{center}
\begin{tabular}{|c|c|c|c|c|c|c|c|c|}
\hline
 	& \textbf{Like}	& \textbf{Dislike}	&\textbf{Point}	&\textbf{Ok}	&\textbf{Slide LR}	&\textbf{Slide RL}	&\textbf{T. a pic}	& \textbf{No ges.} \\
\hline
\hline
 \textbf{Like}   & 80\% & 0 \% & 0 \% & 10\% & 0 \% & 0 \% & 0 \% & 10\% \\
\textbf{Dislike}   & 0 \% & 91\% & 0 \% & 0 \% & 0 \% & 0 \% & 0 \% &  9\% \\
\textbf{Point}  & 0 \% & 0 \% & 91\% & 9\% & 0 \% & 0 \% & 0 \% & 0 \% \\
\textbf{Ok}  & 0 \% & 0 \% & 0 \% & 100\% & 0 \% & 0 \% & 0 \% & 0 \% \\
\textbf{Slide left to right}  & 0 \% & 0 \% & 0 \% & 0 \% & 90\% & 10\% & 0 \% & 0 \% \\
\textbf{Slide right to left}  & 0 \% & 0 \% & 0 \% & 0 \% & 0 \% & 100\% & 0 \% & 0 \% \\
\textbf{Take a picture}  & 0 \% & 0 \% & 0 \% & 0 \% & 0 \% & 0 \% & 100\% & 0 \% \\
\textbf{No gesture}   & 1\% & 0 \% & 0 \% & 1\% & 1\% & 0 \% & 3\% & 93\% \\
\hline
\end{tabular}
\end{center}
\caption{Confusion matrix using the $\delta_0$ per-token loss function. Percentages are rounded to the nearest integer and computed considering sequences of adjacent frames in which a gesture is performed or not.}
\label{losses0}
\end{table*}

\begin{table*}
\begin{center}
\begin{tabular}{|c|c|c|c|c|c|c|c|c|}
\hline
 	& \textbf{Like}	& \textbf{Dislike}	&\textbf{Point}	&\textbf{Ok}	&\textbf{Slide LR}	&\textbf{Slide RL}	&\textbf{T. a pic}	& \textbf{No ges.} \\
\hline
\hline
 \textbf{Like}  & 90\% & 0 \% & 10\% & 0 \% & 0 \% & 0 \% & 0 \% & 0 \% \\
\textbf{Dislike}  & 9\% & 91\% & 0 \% & 0 \% & 0 \% & 0 \% & 0 \% & 0 \% \\
\textbf{Point}  & 0 \% & 0 \% & 91\% & 9\% & 0 \% & 0 \% & 0 \% & 0 \% \\
\textbf{Ok}  & 0 \% & 0 \% & 0 \% & 100\% & 0 \% & 0 \% & 0 \% & 0 \% \\
\textbf{Slide left to right}  & 0 \% & 0 \% & 0 \% & 0 \% & 90\% & 10\% & 0 \% & 0 \% \\
\textbf{Slide right to left} & 0 \% & 0 \% & 0 \% & 0 \% & 0 \% & 100\% & 0 \% & 0 \% \\
\textbf{Take a picture} & 0 \% & 0 \% & 0 \% & 0 \% & 0 \% & 0 \% & 100\% & 0 \% \\
\textbf{No gesture}  & 0 \% & 0 \% & 0 \% & 1 \% & 0 \% & 0 \% & 1 \% & 97\% \\
\hline
\end{tabular}
\end{center}
\caption{Confusion matrix using the $\delta_1$ per-token loss function.  Percentages are rounded to the nearest integer and computed considering sequences of adjacent frames in which a gesture is performed or not.}
\label{losses1}
\end{table*}

We train the Structural SVM classifier using four different gesture sequences from the Maramotti dataset, each five minutes long, and perform test on three sequences taken in front of different artworks. Using our gesture recognition algorithm we get a feature vector for each frame, which is then classified using the Structural SVM classifier. Of course, since sequences contain frames in which the user is not performing any gesture, the classifier has to deal with a \textit{non-gesture} class too.

Table \ref{transemis} shows the accuracy results obtained with various orders of label-label and label-features dependencies, using the $\delta_0$ per-token loss function. As can be seen, high accuracy results can be achieved even with $\tau = 1$ and $\epsilon = 0$, whereas using second order dependencies there is a slight increase in accuracy. On the other hand, this would imply slower training and testing.

Moreover, the $\delta_0$ per-token loss function leads to a confusion matrix where some \textit{non-gesture} examples are classified as gestures (see Table \ref{losses0}). Since false positives can be a major problem in human-machine interfaces, we propose the $\delta_1$ loss function, which increases the penalty when \textit{non-gesture} examples are misclassified and viceversa. Table \ref{losses1} shows that this loss function significantly reduce the confusion between gesture and \textit{non-gesture} sequences.


\section{Implementation}
\lstset{language=C++}

As stated in Section \ref{odroid-intro}, on the Odroid-XU board only one of the two clusters can run at the same time. Since our application is CPU intensive, we choose the A15 cluster, which includes four cores and is the most performant cluster. This is done trough the following command:
\begin{lstlisting}[frame=single]  % Start your code-block

echo performance > /sys/devices/system/cpu/cpu0/cpufreq/scaling_governor
\end{lstlisting}
which basically activates the \verb+performance+ governor as the controller for the big.LITTLE switching mechanism. The \verb+performance+ governor will simply turn off the A7 cluster and turn on the A15 cluster.

Then, having written a sequential implementation of our algorithm using the OpenCV library and our modified version of Joachims's SVM-HMM \cite{joachims} \footnote{This is joint work with Francesco Paci.}, we compile it using \verb+gcc+ and the following options:
\begin{lstlisting}[frame=single]  % Start your code-block

-g -mfpu=neon-vfpv4 -ftree-vectorize -mfloat-abi=hard -mtune=cortex-a15 -marm
\end{lstlisting}

The \verb+-g+ flag tells the compiler to produce debugging information in the operating system's native format, \verb+-mfpu=neon+  specifies what floating-point hardware (or hardware emulation) is available on the target, \verb+-ftree-vectorize+ enables the auto-vectorizer (which will try to use SIMD instructions, when possible), \verb+-mfloat-abi=hard+ allows generation of floating-point instructions and uses FPU-specific calling conventions, and \verb+-mtune=cortex-a15+ tunes the performance of the compiler for the A15 target.

\subsection{Optimizations}
\begin{figure}[t!]
\centering
\includegraphics[width=1.5\linewidth,angle=90]{Figures/grafico_prestazioni.jpg}
\caption{A section of the Valgrind Call Graph of the first sequential version. As can be seen, the optical flow takes more than the 50\% of the execution time.}
\label{valgrind}
\end{figure}

This first sequential version runs at 4.3 frames/s on $160\times 120$ frames, and therefore needs 234 ms to elaborate each frame. Having profiled the application with Valgrind \cite{nethercote2007valgrind}, we observe that the main bottleneck of this implementation is the multi-scale Farneback's optical flow, which requires 141 ms for each frame to run (see Figure \ref{valgrind} for a section of the Valgrind call graph). Farneback's algorithm is executed two times for each frame, since the optical flow is calculated both on the original frame on the warped one, so we exploit OpenMP parallel sections to run these two calls simultaneously on two different threads. Then, we use Neon intrinsics in order to exploit the SIMD capabilities of our CPU and thus gain a better performance on each thread.

A complete explanation of the optimizations made is beyond the purpose of this chapter, but, as an example, let's consider this for loop, inside the OpenCV implementation of Farneback's optical flow\footnote{Source code is available at: \url{https://github.com/Itseez/opencv/blob/0224a20ff6d0cf051cf818efb364048a2dcb716d/modules/video/src/optflowgf.cpp}}, which is responsible of 51 of the 234 ms:
\begin{lstlisting}[frame=single]  % Start your code-block

for( ; x < width*5; x++ ) {
	float s0 = srow[m][x]*kernel[0];
	for( i = 1; i <= m; i++ )
		s0 += (srow[m+i][x] + srow[m-i][x])*kernel[i];
	vsum[x] = s0;
}
\end{lstlisting}

Once optimized with Neon SIMD, the previous code becomes:
\begin{lstlisting}[frame=single]  % Start your code-block

int xstart = x;
float kernelext0[4];
fill_n(kernelext0,4,kernel[0]);
for( ; x < width*5-4; x+=4 ) {
           vst1q_f32(vsum+x, vmulq_f32(vld1q_f32(srow[m]+x),
		vld1q_f32(kernelext0)));
}
float a[width*5];
float kernelext[4];
for( i = 1; i <= m; i++ ) {
	fill_n(kernelext, 4, kernel[i]);
           for (x=xstart; x<width*5-4; x+=4) {
                        vst1q_f32(a+x, vaddq_f32(vld1q_f32(srow[m+i]+x), 
			vld1q_f32(srow[m-i]+x)));
                        vst1q_f32(vsum+x, vmlaq_f32(vld1q_f32(vsum+x), 
			vld1q_f32(a+x), vld1q_f32(kernelext)));
           }
}
\end{lstlisting}
As can be seen, the two for loops have been inverted and four floats are processed at each iteration now. The new code block takes only 20 ms to run.


Having included several other OpenMP/SIMD optimizations, our code runs at 10 frames/s on the A15 cluster, still not enough for real-time. Moreover, our algorithm needs an high frame rate in order to compute significant trajectories. Being Farneback's algorithm our main bottleneck, we could turn our attention to the PowerVR GPU, and use the OpenCL implementation of Farneback's algorithm included in OpenCV, for instance. Unfortunately, Hardkernel has not yet released an Ubuntu kernel that supports the PowerVR GPU\footnote{See: \url{http://forum.odroid.com/viewtopic.php?f=61&t=2236}.}, so the only way we have to increase speed is to further reduce the frame size and keep only one level of the spatial pyramid. Having reduced the frame size to $113\times 85$ the code can run at 14 fps, which is quite a good result, since trajectories can still be extracted with good accuracy and the overall recognition performance is only slightly affected: in fact, we have observed a $5\%$ drop in recognition accuracy.

\section{Some applications}
During the process of adapting our approach to build a real-time gesture recognizer we have implemented and tested two applications, both of which have required a careful tuning and test phase: an \textit{ego-vision jacket}, which basically is a jacket that embeds our developer board and a camera, placed on the chest, and a \textit{gesture-based interface} for desktop applications.

\textbf{Ego-Vision Jacket}: Embedding a wearable camera and a board in a jacket has required some tailoring work: the lens of the camera has been placed on a butthole, sewed on the chest, and the board in a custom designed pocket, made with breathable fabric. Furthermore, a battery has been developed to make the board completely wireless. 

\begin{figure}[t!]
\centering
\includegraphics[width=1.2\linewidth,angle=90]{Figures/consumi.pdf}
\caption{CPU Frequency, Power consumption and CPU Temperature during a two hours execution of our algorithm, inside the pocket of Ego-vision Jacket.}
\label{jacket}
\end{figure}

Of course here the main technical issue is overheating, since the board has to stay inside a pocket: for this reason, we extensively measured the CPU temperature with different workload conditions and external temperatures. Results during a two hours execution of our algorithm shows that in fact the board temperature is remarkably higher than in normal conditions, but the embedded fan is still able to maintain the cores well below their maximum allowed temperature, that is $80\text{ }^{\circ}\text{C}$ (see Fig. \ref{jacket}).

Our \textit{ego-vision jacket} is the first prototype of a future high technological jacket we are going to develop, which will include other sensors, like a GPS antenna, and which will be fully connected to the internet (via EDGE/UMTS), to local area networks (through the Ethernet port or through the Wifi module) and to the user's smartphone via Bluetooth. We plan to use such jackets, in conjunction with augmented-reality algorithms, to enhance historical city visits.

\textbf{Gesture-based interface}
This is basically a gesture-based controller for Power Point presentations or others desktop applications, that gives the user the ability to control a GUI using his gestures. Commands are passed through a simple socket, and then transformed in mouse or keyboards events. We exploit a client-server model, where our client is the Odroid-XU board, and the computer hosting the presentations acts as a server.

The board automatically connects to the remote server, via the following lines:
\begin{lstlisting}[frame=single]
int sockfd;
struct sockaddr_in serv_addr;
char buffer[1];
sockfd = socket(AF_INET, SOCK_STREAM, 0);
if (sockfd < 0) {
  cerr << "Error opening socket";
  exit(EXIT_FAILURE);
}
memset(&serv_addr, 0, sizeof(serv_addr));
serv_addr.sin_family = AF_INET;
serv_addr.sin_addr.s_addr = inet_addr(ip_address);
serv_addr.sin_port = htons(atoi(port_number));

if (connect(sockfd, (struct sockaddr*) &serv_addr, sizeof(serv_addr)) < 0) {
  cerr << "Unable to connect";
  exit(EXIT_FAILURE);
}
\end{lstlisting}
where \verb+ip_address+ and \verb+port_number+ are the server IP address port number. Once a gesture is recognized, the client sends the corresponding command on the socket, using the \verb+send+ primitive. In our implementation commands are coded with an unsigned char, thus allowing 255 different commands. Similarly, the server creates a socket and listens on \verb+port_number+. It then transforms the command into a sequence of mouse/keyboard events using \verb+xdotool+. For example, if a gesture corresponds to pressing the Enter key, the following command is executed:
\begin{lstlisting}[frame=single]
xdotool key KP_Enter
\end{lstlisting}
or, if the gesture corresponds to a mouse click, the server calls:
\begin{lstlisting}[frame=single]
xdotool click 1
\end{lstlisting}

Of course, more complex mouse/keyboard actions can be defined. In our demo (see Figure \ref{fig:projector}), we used a set of three gestures: the \textit{Point} gesture, to trigger an animation, and two \textit{Slide} gestures, to move backwards and forwards in a Power Point presentation.

\begin{figure}
\centering
\subfigure[The \textit{point} gesture reveals a description of the current artwork.]{
	\includegraphics[width=.65\columnwidth]{Figures/projector.jpg}
} \\
\subfigure[\textit{Slide} gestures let the user move forward and backwards.]{
	\includegraphics[width=.65\columnwidth]{Figures/projector2.jpg}
} \\
\caption{Gestures let the user control a virtual museum interface. A demo video is available at \url{http://www.lorenzobaraldi.com/files/EgoVision_HCI.wmv}}
\label{fig:projector}
\end{figure}